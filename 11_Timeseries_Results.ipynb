{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from mittens import GloVe\n",
    "\n",
    "# import glove\n",
    "# from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, merge, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, merge\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data ##\n",
    "\n",
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_model_results = [{\"modelName\":\"GRU-256-los_3-best_model.hdf5\", \"results\":\"results/256-GRU-los_3-10-new.p\", \"problem\":\"los_3\"},{\"modelName\":\"GRU-256-los_7-best_model.hdf5\", \"results\":\"results/256-GRU-los_7-10-new.p\", \"problem\":\"los_7\"},{\"modelName\":\"GRU-256-mort_hosp-best_model.hdf5\", \"results\":\"results/256-GRU-mort_hosp-10-new.p\", \"problem\":\"mort_hosp\"},{\"modelName\":\"GRU-256-mort_icu-best_model.hdf5\", \"results\":\"results/256-GRU-mort_icu-10-new.p\", \"problem\":\"mort_icu\"},{\"modelName\":\"LSTM-256-los_3-best_model.hdf5\", \"results\":\"results/256-LSTM-los_3-10-new.p\", \"problem\":\"los_3\"},{\"modelName\":\"LSTM-256-los_7-best_model.hdf5\", \"results\":\"results/256-LSTM-los_7-10-new.p\", \"problem\":\"los_7\"},{\"modelName\":\"LSTM-256-mort_hosp-best_model.hdf5\", \"results\":\"results/256-LSTM-mort_hosp-10-new.p\", \"problem\":\"mort_hosp\"},{\"modelName\":\"LSTM-256-mort_icu-best_model.hdf5\", \"results\":\"results/256-LSTM-mort_icu-10-new.p\", \"problem\":\"mort_icu\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_timeseries(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def printComparison(modelName, resultsFile, model_output):\n",
    "    stats = pd.read_pickle(resultsFile)\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"modelName: {modelName}, auc: {model_output['auc']==stats['auc']}, auprc: {model_output['auprc']==stats['auprc']}, acc: {model_output['acc']==stats['acc']} F1: {model_output['F1']==stats['F1']}\")\n",
    "    print(stats)\n",
    "\n",
    "def get_model_output(problem, probs, predictions):\n",
    "    ground_truth = y_test[problem].values\n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    model_output = {}    \n",
    "    model_output['auc'] = auc\n",
    "    model_output['auprc'] = auprc\n",
    "    model_output['acc'] = acc\n",
    "    model_output['F1'] = F1\n",
    "    return model_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "modelName: GRU-256-los_3-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.6893080498763451, 'auprc': 0.6273247606110519, 'acc': 0.656163753449862, 'F1': 0.5358584290592984}\n",
      "--------------------------\n",
      "modelName: GRU-256-los_7-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.7242005077570872, 'auprc': 0.20549892835355515, 'acc': 0.9174333026678932, 'F1': 0.021798365122615803}\n",
      "--------------------------\n",
      "modelName: GRU-256-mort_hosp-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.8739894484095516, 'auprc': 0.5576770818641951, 'acc': 0.9139834406623735, 'F1': 0.42813455657492355}\n",
      "--------------------------\n",
      "modelName: GRU-256-mort_icu-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.8771797993039626, 'auprc': 0.4832968288657088, 'acc': 0.9362925482980681, 'F1': 0.3661327231121282}\n",
      "--------------------------\n",
      "modelName: LSTM-256-los_3-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.686427513994812, 'auprc': 0.6306882380400414, 'acc': 0.6584636614535418, 'F1': 0.5460103943748089}\n",
      "--------------------------\n",
      "modelName: LSTM-256-los_7-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.7201671601630227, 'auprc': 0.19695673431079366, 'acc': 0.9174333026678932, 'F1': 0.011019283746556472}\n",
      "--------------------------\n",
      "modelName: LSTM-256-mort_hosp-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.8799134284959028, 'auprc': 0.564574876029393, 'acc': 0.9128334866605335, 'F1': 0.44017725258493345}\n",
      "--------------------------\n",
      "modelName: LSTM-256-mort_icu-best_model.hdf5, auc: True, auprc: True, acc: True F1: True\n",
      "{'auc': 0.88027217254241, 'auprc': 0.4923985677752011, 'acc': 0.9385924563017479, 'F1': 0.4282655246252676}\n"
     ]
    }
   ],
   "source": [
    "## loop through each model and print comparision results ##\n",
    "for model_data in timeseries_model_results:\n",
    "    model = load_model(model_data['modelName'], custom_objects={'_initializer': glorot_uniform()})\n",
    "    probs, predictions = make_prediction_timeseries(model, x_test_lstm)\n",
    "    model_output = get_model_output(model_data['problem'], probs, predictions)\n",
    "    printComparison(model_data['modelName'], model_data['results'], model_output)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e6498bf1da40c9c754b0c5729dd8a90892bd6d66680dcd2d9d7c662adad743e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py3713')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
