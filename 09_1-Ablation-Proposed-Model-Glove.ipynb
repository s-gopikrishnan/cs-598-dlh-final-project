{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input, concatenate, merge, Activation, Concatenate, LSTM, GRU\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\n",
    "from keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, merge\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "from keras.backend.tensorflow_backend import set_session, clear_session, get_session\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_ner = \"new\"\n",
    "\n",
    "x_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\n",
    "x_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\n",
    "x_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n",
    "\n",
    "y_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\n",
    "y_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\n",
    "y_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n",
    "\n",
    "\n",
    "ner_glove = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_Glove_dict.pkl\")\n",
    "\n",
    "train_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\n",
    "dev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\n",
    "test_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_cnn(model, test_data):\n",
    "    probs = model.predict(test_data)\n",
    "    y_pred = [1 if i>=0.5 else 0 for i in probs]\n",
    "    return probs, y_pred\n",
    "\n",
    "def save_scores_cnn(predictions, probs, ground_truth, \n",
    "                          \n",
    "                          embed_name, problem_type, iteration, hidden_unit_size,\n",
    "                          \n",
    "                          sequence_name, type_of_ner):\n",
    "    \n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    result_dict = {}    \n",
    "    result_dict['auc'] = auc\n",
    "    result_dict['auprc'] = auprc\n",
    "    result_dict['acc'] = acc\n",
    "    result_dict['F1'] = F1\n",
    "\n",
    "    result_path = \"results/cnn/\"\n",
    "    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n",
    "    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-cnn-.p\"\n",
    "    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n",
    "\n",
    "    print(auc, auprc, acc, F1)\n",
    "    \n",
    "def print_scores_cnn(predictions, probs, ground_truth, model_name, problem_type, iteration, hidden_unit_size):\n",
    "    auc = roc_auc_score(ground_truth, probs)\n",
    "    auprc = average_precision_score(ground_truth, probs)\n",
    "    acc   = accuracy_score(ground_truth, predictions)\n",
    "    F1    = f1_score(ground_truth, predictions)\n",
    "    \n",
    "    print (\"AUC: \", auc, \"AUPRC: \", auprc, \"F1: \", F1)\n",
    "    \n",
    "def get_subvector_data(size, embed_name, data):\n",
    "    if embed_name == \"concat\":\n",
    "        vector_size = 200\n",
    "    else:\n",
    "        vector_size = 300\n",
    "\n",
    "    x_data = {}\n",
    "    for k, v in data.items():\n",
    "        number_of_additional_vector = len(v) - size\n",
    "        vector = []\n",
    "        for i in v:\n",
    "            vector.append(i)\n",
    "        if number_of_additional_vector < 0: \n",
    "            number_of_additional_vector = np.abs(number_of_additional_vector)\n",
    "\n",
    "            temp = vector[:size]\n",
    "            for i in range(0, number_of_additional_vector):\n",
    "                temp.append(np.zeros(vector_size))\n",
    "            x_data[k] = np.asarray(temp)\n",
    "        else:\n",
    "            x_data[k] = np.asarray(vector[:size])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "def proposedmodel(layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n",
    "    if embedding_name == \"concat\":\n",
    "        input_dimension = 200\n",
    "    else:\n",
    "        input_dimension = 300\n",
    "\n",
    "    sequence_input = Input(shape=(24,104))\n",
    "\n",
    "    input_img = Input(shape=(ner_limit, input_dimension), name = \"cnn_input\")\n",
    "\n",
    "    convs = []\n",
    "    filter_sizes = [2,3,4]\n",
    "\n",
    "\n",
    "\n",
    "    text_conv1d = Conv1D(filters=num_filter, kernel_size=3, \n",
    "                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu', \n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer() )(input_img)\n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*2, kernel_size=3, \n",
    "                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu',\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer())(text_conv1d)   \n",
    "    \n",
    "    text_conv1d = Conv1D(filters=num_filter*3, kernel_size=3, \n",
    "                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu',\n",
    "                        kernel_initializer=tf.contrib.layers.xavier_initializer())(text_conv1d)   \n",
    "\n",
    "    \n",
    "    #concat_conv = keras.layers.Concatenate()([text_conv1d, text_conv1d_2, text_conv1d_3])\n",
    "    text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n",
    "    #text_embeddings = Dense(128, activation=\"relu\")(text_embeddings)\n",
    "    \n",
    "    if layer_name == \"GRU\":\n",
    "        x = GRU(number_of_unit)(sequence_input)\n",
    "    elif layer_name == \"LSTM\":\n",
    "        x = LSTM(number_of_unit)(sequence_input)\n",
    "\n",
    "    #concatenated = keras.layers.Concatenate()([x, text_embeddings])\n",
    "    \n",
    "    # concatenated = merge([x, text_embeddings], mode='concat', concat_axis=1)\n",
    "    concatenated = keras.layers.concatenate([x, text_embeddings], axis=1)\n",
    "\n",
    "    concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    concatenated = Dropout(0.2)(concatenated)\n",
    "    #concatenated = Dense(256, activation='relu')(concatenated)\n",
    "    #concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    \n",
    "    #concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n",
    "    preds = Dense(1, activation='sigmoid',use_bias=False,\n",
    "                         kernel_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                  kernel_regularizer=logits_regularizer)(concatenated)\n",
    "    \n",
    "    \n",
    "    #opt = Adam(lr=1e-4, decay = 0.01)\n",
    "    \n",
    "    opt = Adam(lr=1e-3, decay = 0.01)\n",
    "    \n",
    "    #opt = Adam(lr=0.001)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, input_img], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  Glove\n",
      "=============================\n",
      "Iteration number:  1\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/selvaganapathyt/opt/anaconda3/envs/test_tf/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 20:00:15.223317: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-01 20:00:15.224071: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 10. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15119/15119 [==============================] - 52s 3ms/step - loss: 0.2729 - acc: 0.9041 - val_loss: 0.2418 - val_acc: 0.9137\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24184, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.2316 - acc: 0.9169 - val_loss: 0.2363 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24184 to 0.23630, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 44s 3ms/step - loss: 0.2181 - acc: 0.9210 - val_loss: 0.2387 - val_acc: 0.9095\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.23630\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 48s 3ms/step - loss: 0.2089 - acc: 0.9248 - val_loss: 0.2401 - val_acc: 0.9090\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.23630\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1998 - acc: 0.9293 - val_loss: 0.2372 - val_acc: 0.9113\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.23630\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.1976 - acc: 0.9301 - val_loss: 0.2368 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23630\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.1956 - acc: 0.9318 - val_loss: 0.2367 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23630\n",
      "AUC:  0.877752490106577 AUPRC:  0.5760819714510136 F1:  0.470756062767475\n",
      "0.8776501779064588 0.5774841000299238 0.9143121815655396 0.4510385756676558\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 49s 3ms/step - loss: 0.2078 - acc: 0.9335 - val_loss: 0.1751 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17507, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1682 - acc: 0.9440 - val_loss: 0.1739 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17507 to 0.17393, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.1552 - acc: 0.9467 - val_loss: 0.1710 - val_acc: 0.9459\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17393 to 0.17096, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1468 - acc: 0.9491 - val_loss: 0.1691 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.17096 to 0.16905, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.1404 - acc: 0.9513 - val_loss: 0.1706 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16905\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 45s 3ms/step - loss: 0.1339 - acc: 0.9524 - val_loss: 0.1724 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16905\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 45s 3ms/step - loss: 0.1274 - acc: 0.9547 - val_loss: 0.1730 - val_acc: 0.9454\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16905\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.1264 - acc: 0.9550 - val_loss: 0.1738 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16905\n",
      "Epoch 9/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.1247 - acc: 0.9561 - val_loss: 0.1739 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.16905\n",
      "AUC:  0.8860923570369238 AUPRC:  0.5317150007331767 F1:  0.46625766871165647\n",
      "0.8871977589198915 0.5318498580462121 0.9404817044928208 0.44251626898047725\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.6500 - acc: 0.6395 - val_loss: 0.6172 - val_acc: 0.6776\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61723, saving model to 64-basiccnn1d-Glove-los_3-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.6161 - acc: 0.6750 - val_loss: 0.6105 - val_acc: 0.6776\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61723 to 0.61046, saving model to 64-basiccnn1d-Glove-los_3-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 52s 3ms/step - loss: 0.5995 - acc: 0.6889 - val_loss: 0.6082 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61046 to 0.60823, saving model to 64-basiccnn1d-Glove-los_3-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 49s 3ms/step - loss: 0.5845 - acc: 0.7062 - val_loss: 0.6089 - val_acc: 0.6776\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.60823\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.5726 - acc: 0.7182 - val_loss: 0.6103 - val_acc: 0.6701\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.60823\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.5580 - acc: 0.7272 - val_loss: 0.6094 - val_acc: 0.6766\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.60823\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 41s 3ms/step - loss: 0.5557 - acc: 0.7311 - val_loss: 0.6093 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.60823\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 44s 3ms/step - loss: 0.5524 - acc: 0.7308 - val_loss: 0.6098 - val_acc: 0.6762\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.60823\n",
      "AUC:  0.7071170898321375 AUPRC:  0.6456260807478427 F1:  0.5680119581464873\n",
      "0.7056051823563201 0.644296427483847 0.6681333950903195 0.539376406300225\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.2748 - acc: 0.9186 - val_loss: 0.2605 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26047, saving model to 64-basiccnn1d-Glove-los_7-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.2470 - acc: 0.9218 - val_loss: 0.2560 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26047 to 0.25596, saving model to 64-basiccnn1d-Glove-los_7-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.2379 - acc: 0.9220 - val_loss: 0.2584 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.25596\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.2316 - acc: 0.9227 - val_loss: 0.2572 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.25596\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.2253 - acc: 0.9229 - val_loss: 0.2560 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25596\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.2239 - acc: 0.9229 - val_loss: 0.2561 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25596\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.2233 - acc: 0.9229 - val_loss: 0.2561 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25596\n",
      "AUC:  0.7302115673681087 AUPRC:  0.2157469378356459 F1:  0.04891304347826087\n",
      "0.7300011418993018 0.21223103666912999 0.9189439555349699 0.016853932584269666\n",
      "Iteration number:  2\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 51s 3ms/step - loss: 0.2743 - acc: 0.9023 - val_loss: 0.2403 - val_acc: 0.9160\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24034, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 42s 3ms/step - loss: 0.2313 - acc: 0.9178 - val_loss: 0.2358 - val_acc: 0.9118\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24034 to 0.23582, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.2180 - acc: 0.9216 - val_loss: 0.2335 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23582 to 0.23350, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 51s 3ms/step - loss: 0.2082 - acc: 0.9239 - val_loss: 0.2367 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.23350\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.2009 - acc: 0.9265 - val_loss: 0.2336 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.23350\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.1920 - acc: 0.9307 - val_loss: 0.2327 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23350 to 0.23275, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 45s 3ms/step - loss: 0.1897 - acc: 0.9318 - val_loss: 0.2329 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23275\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 53s 4ms/step - loss: 0.1884 - acc: 0.9304 - val_loss: 0.2334 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.23275\n",
      "Epoch 9/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1869 - acc: 0.9326 - val_loss: 0.2334 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.23275\n",
      "Epoch 10/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1876 - acc: 0.9322 - val_loss: 0.2334 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.23275\n",
      "Epoch 11/100\n",
      "15119/15119 [==============================] - 48s 3ms/step - loss: 0.1869 - acc: 0.9325 - val_loss: 0.2334 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.23275\n",
      "AUC:  0.8746922988383673 AUPRC:  0.5745145017213182 F1:  0.46648044692737434\n",
      "0.8743828600374662 0.5742523262372794 0.9143121815655396 0.46991404011461324\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 49s 3ms/step - loss: 0.2091 - acc: 0.9325 - val_loss: 0.1722 - val_acc: 0.9421\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17217, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1672 - acc: 0.9454 - val_loss: 0.1706 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17217 to 0.17063, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.1555 - acc: 0.9476 - val_loss: 0.1672 - val_acc: 0.9435\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.17063 to 0.16720, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 47s 3ms/step - loss: 0.1460 - acc: 0.9498 - val_loss: 0.1681 - val_acc: 0.9426\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.16720\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 50s 3ms/step - loss: 0.1400 - acc: 0.9513 - val_loss: 0.1688 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.16720\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 52s 3ms/step - loss: 0.1318 - acc: 0.9535 - val_loss: 0.1692 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16720\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 55s 4ms/step - loss: 0.1308 - acc: 0.9538 - val_loss: 0.1701 - val_acc: 0.9445\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.16720\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 49s 3ms/step - loss: 0.1297 - acc: 0.9547 - val_loss: 0.1701 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.16720\n",
      "AUC:  0.8828723418514455 AUPRC:  0.5313730101196512 F1:  0.45991561181434604\n",
      "0.8834242543465081 0.5307307384153398 0.9418712366836498 0.46021505376344096\n",
      "Problem type:  los_3\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 55s 4ms/step - loss: 0.6487 - acc: 0.6414 - val_loss: 0.6291 - val_acc: 0.6645\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62909, saving model to 64-basiccnn1d-Glove-los_3-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 43s 3ms/step - loss: 0.6155 - acc: 0.6757 - val_loss: 0.6169 - val_acc: 0.6808\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62909 to 0.61694, saving model to 64-basiccnn1d-Glove-los_3-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 44s 3ms/step - loss: 0.5995 - acc: 0.6905 - val_loss: 0.6120 - val_acc: 0.6785\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61694 to 0.61196, saving model to 64-basiccnn1d-Glove-los_3-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 45s 3ms/step - loss: 0.5843 - acc: 0.7057 - val_loss: 0.6128 - val_acc: 0.6804\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.61196\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 49s 3ms/step - loss: 0.5692 - acc: 0.7157 - val_loss: 0.6140 - val_acc: 0.6771\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.61196\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 62s 4ms/step - loss: 0.5539 - acc: 0.7322 - val_loss: 0.6121 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.61196\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 48s 3ms/step - loss: 0.5508 - acc: 0.7305 - val_loss: 0.6128 - val_acc: 0.6808\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.61196\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 48s 3ms/step - loss: 0.5473 - acc: 0.7348 - val_loss: 0.6129 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.61196\n",
      "AUC:  0.7073721037741407 AUPRC:  0.6489179886468752 F1:  0.5693084001187295\n",
      "0.7078216903975261 0.6490449206914757 0.6667438628994905 0.5444761000316556\n",
      "Problem type:  los_7\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 60s 4ms/step - loss: 0.2727 - acc: 0.9214 - val_loss: 0.2588 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25883, saving model to 64-basiccnn1d-Glove-los_7-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 68s 5ms/step - loss: 0.2463 - acc: 0.9218 - val_loss: 0.2582 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25883 to 0.25823, saving model to 64-basiccnn1d-Glove-los_7-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 66s 4ms/step - loss: 0.2372 - acc: 0.9217 - val_loss: 0.2573 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.25823 to 0.25727, saving model to 64-basiccnn1d-Glove-los_7-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 75s 5ms/step - loss: 0.2313 - acc: 0.9221 - val_loss: 0.2568 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.25727 to 0.25685, saving model to 64-basiccnn1d-Glove-los_7-best_model.hdf5\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 61s 4ms/step - loss: 0.2251 - acc: 0.9229 - val_loss: 0.2575 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.25685\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 57s 4ms/step - loss: 0.2199 - acc: 0.9235 - val_loss: 0.2576 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.25685\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 44s 3ms/step - loss: 0.2136 - acc: 0.9240 - val_loss: 0.2574 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.25685\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 54s 4ms/step - loss: 0.2129 - acc: 0.9246 - val_loss: 0.2580 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.25685\n",
      "Epoch 9/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.2119 - acc: 0.9247 - val_loss: 0.2580 - val_acc: 0.9253\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.25685\n",
      "AUC:  0.7266515706142628 AUPRC:  0.2170160696606515 F1:  0.05434782608695653\n",
      "0.7230908556847554 0.2181636031322611 0.9201018990273274 0.03899721448467967\n",
      "Iteration number:  3\n",
      "Problem type:  mort_hosp\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 69s 5ms/step - loss: 0.2727 - acc: 0.9052 - val_loss: 0.2390 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.23897, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 66s 4ms/step - loss: 0.2298 - acc: 0.9180 - val_loss: 0.2399 - val_acc: 0.9146\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.23897\n",
      "Epoch 3/100\n",
      "15119/15119 [==============================] - 50s 3ms/step - loss: 0.2154 - acc: 0.9231 - val_loss: 0.2380 - val_acc: 0.9104\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.23897 to 0.23796, saving model to 64-basiccnn1d-Glove-mort_hosp-best_model.hdf5\n",
      "Epoch 4/100\n",
      "15119/15119 [==============================] - 53s 3ms/step - loss: 0.2052 - acc: 0.9264 - val_loss: 0.2399 - val_acc: 0.9109\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.23796\n",
      "Epoch 5/100\n",
      "15119/15119 [==============================] - 53s 4ms/step - loss: 0.1972 - acc: 0.9304 - val_loss: 0.2389 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.23796\n",
      "Epoch 6/100\n",
      "15119/15119 [==============================] - 60s 4ms/step - loss: 0.1878 - acc: 0.9325 - val_loss: 0.2389 - val_acc: 0.9123\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23796\n",
      "Epoch 7/100\n",
      "15119/15119 [==============================] - 50s 3ms/step - loss: 0.1866 - acc: 0.9329 - val_loss: 0.2394 - val_acc: 0.9151\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.23796\n",
      "Epoch 8/100\n",
      "15119/15119 [==============================] - 46s 3ms/step - loss: 0.1855 - acc: 0.9335 - val_loss: 0.2394 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.23796\n",
      "AUC:  0.8762133587482769 AUPRC:  0.5742286717307644 F1:  0.4718792866941015\n",
      "0.8783185434746223 0.5790589168178282 0.9115331171838814 0.47814207650273216\n",
      "Problem type:  mort_icu\n",
      "__________________\n",
      "Train on 15119 samples, validate on 2143 samples\n",
      "Epoch 1/100\n",
      "15119/15119 [==============================] - 65s 4ms/step - loss: 0.2039 - acc: 0.9350 - val_loss: 0.1695 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16948, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 2/100\n",
      "15119/15119 [==============================] - 60s 4ms/step - loss: 0.1666 - acc: 0.9436 - val_loss: 0.1670 - val_acc: 0.9468\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16948 to 0.16696, saving model to 64-basiccnn1d-Glove-mort_icu-best_model.hdf5\n",
      "Epoch 3/100\n",
      "15104/15119 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9491"
     ]
    }
   ],
   "source": [
    "# embedding_types = ['word2vec', 'fasttext', 'concat']\n",
    "embedding_types = ['Glove']\n",
    "\n",
    "# embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n",
    "embedding_dict = [ner_glove]\n",
    "\n",
    "target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n",
    "\n",
    "num_epoch = 100\n",
    "model_patience = 5\n",
    "monitor_criteria = 'val_loss'\n",
    "#monitor_criteria = 'val_acc'\n",
    "batch_size = 64\n",
    "\n",
    "filter_number = 32\n",
    "ner_representation_limit = 64\n",
    "activation_func = \"relu\"\n",
    "\n",
    "sequence_model = \"GRU\"\n",
    "sequence_hidden_unit = 256\n",
    "\n",
    "maxiter = 11\n",
    "for embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n",
    "    print (\"Embedding: \", embed_name)\n",
    "    print(\"=============================\")\n",
    "    \n",
    "    temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n",
    "    tem_dev_ner = dict((k, embed_dict[k]) for k in dev_ids)\n",
    "    temp_test_ner = dict((k, embed_dict[k]) for k in test_ids)\n",
    "\n",
    "    x_train_dict = {}\n",
    "    x_dev_dict = {}\n",
    "    x_test_dict = {}\n",
    "\n",
    "    x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n",
    "    x_dev_dict = get_subvector_data(ner_representation_limit, embed_name, tem_dev_ner)\n",
    "    x_test_dict = get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n",
    "\n",
    "    x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n",
    "    x_dev_dict_sorted = collections.OrderedDict(sorted(x_dev_dict.items()))\n",
    "    x_test_dict_sorted = collections.OrderedDict(sorted(x_test_dict.items()))\n",
    "\n",
    "    # x_train_ner = np.asarray(x_train_dict_sorted.values())\n",
    "    # x_dev_ner = np.asarray(x_dev_dict_sorted.values())\n",
    "    # x_test_ner = np.asarray(x_test_dict_sorted.values())\n",
    "\n",
    "    x_train_ner = np.array(list(x_train_dict_sorted.values())) \n",
    "    x_dev_ner = np.array(list(x_dev_dict_sorted.values()))\n",
    "    x_test_ner = np.array(list(x_test_dict_sorted.values()))\n",
    "\n",
    "        \n",
    "    for iteration in range(1,maxiter):\n",
    "        print (\"Iteration number: \", iteration)\n",
    "    \n",
    "        for each_problem in target_problems:\n",
    "            print (\"Problem type: \", each_problem)\n",
    "            print (\"__________________\")\n",
    "            \n",
    "            \n",
    "            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n",
    "            \n",
    "            best_model_name = str(ner_representation_limit)+\"-basiccnn1d-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n",
    "            \n",
    "            checkpoint = ModelCheckpoint(best_model_name, monitor=monitor_criteria, verbose=1,\n",
    "                save_best_only=True, mode='min')\n",
    "            \n",
    "            reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, factor=0.2,\n",
    "                              patience=2, min_lr=0.00001, epsilon=1e-4, mode='min')\n",
    "            \n",
    "\n",
    "            callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n",
    "            \n",
    "            #model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\n",
    "            model = proposedmodel(sequence_model, sequence_hidden_unit, \n",
    "                               embed_name, ner_representation_limit,filter_number)\n",
    "            model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n",
    "                      validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, batch_size=batch_size)\n",
    "            \n",
    "            \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
    "            print_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration, sequence_hidden_unit)\n",
    "            \n",
    "            model.load_weights(best_model_name)\n",
    "                      \n",
    "            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n",
    "            save_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration,\n",
    "                            sequence_hidden_unit, sequence_model, type_of_ner)\n",
    "            del model\n",
    "            clear_session()\n",
    "            gc.collect()\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aac8dec30b2392ddea3a2b04b3b1a1a9ab9ac95d8d7217d56add55303069cbca"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
